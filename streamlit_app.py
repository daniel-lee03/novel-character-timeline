# -*- coding: utf-8 -*-
# ì‹¤í–‰: streamlit run --server.port 3000 --server.address 0.0.0.0 streamlit_app.py

import os
import re
from typing import List, Dict, Any, Tuple

import requests
import pandas as pd
import plotly.graph_objects as go
import streamlit as st
from dotenv import load_dotenv

# --------------------------------
# í™˜ê²½ ë¡œë“œ
# --------------------------------
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN", "").strip()

st.set_page_config(
    page_title="ì†Œì„¤ ì¸ë¬¼ ë“±ì¥ ë¹„ìœ¨ íƒ€ì„ë¼ì¸",
    layout="wide",
    page_icon="ğŸ“š",
)

st.title("ğŸ“š ì†Œì„¤ ì¸ë¬¼ ë“±ì¥ ë¹„ìœ¨ íƒ€ì„ë¼ì¸")
st.caption("Streamlit Â· GitHub Codespaces Â· (ì„ íƒ) Hugging Face Inference API")

# --------------------------------
# ìœ í‹¸
# --------------------------------
HANGUL_ALNUM = r"[ê°€-í£A-Za-z0-9]"

def split_sentences_kr(text: str) -> List[str]:
    """
    í•œêµ­ì–´ì— ê°„ë‹¨íˆ ì‘ë™í•˜ëŠ” ë¬¸ì¥ ë¶„ë¦¬ê¸°(ê°€ë²¼ìš´ ì •ê·œì‹ ê¸°ë°˜).
    ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ì¤„ë°”ê¿ˆ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ í›„ íŠ¸ë¦¼.
    """
    text = re.sub(r"\r\n|\r", "\n", text)
    chunks = re.split(r"(?<=[.!?â€¦ã€‚ï¼ï¼Ÿ])\s+|\n{2,}", text)
    sents = []
    for c in chunks:
        c = c.strip()
        if not c:
            continue
        sents.extend([s.strip() for s in re.split(r"\n+", c) if s.strip()])
    return sents

def make_windows(items: List[str], window: int, step: int) -> List[Tuple[int, int]]:
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬ê°„(index start,end exclusive) ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
    spans = []
    n = len(items)
    if n == 0:
        return spans
    i = 0
    while i < n:
        j = min(i + window, n)
        spans.append((i, j))
        if j == n:
            break
        i += step
    return spans

def compile_alias_regex(aliases: List[str]) -> re.Pattern:
    """
    ë³„ì¹­ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì•ˆì „í•œ ì •ê·œì‹ìœ¼ë¡œ ì»´íŒŒì¼.
    í•œêµ­ì–´ ë‹¨ì–´ ê²½ê³„ ìœ ì‚¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì£¼ë³€ì— í•œê¸€/ì˜ë¬¸/ìˆ«ì ì•„ë‹Œ ê²½ìš°ë§Œ ë§¤ì¹­.
    """
    escaped = [re.escape(a.strip()) for a in aliases if a.strip()]
    if not escaped:
        escaped = ["a^"]  # ì ˆëŒ€ ë§¤ì¹­ë˜ì§€ ì•Šë„ë¡
    pattern = rf"(?<!{HANGUL_ALNUM})(?:{'|'.join(escaped)})(?!{HANGUL_ALNUM})"
    return re.compile(pattern)

def count_mentions(text: str, alias_re: re.Pattern) -> int:
    return len(alias_re.findall(text))

def soft_normalize_whitespace(text: str) -> str:
    t = re.sub(r"[ \t]+", " ", text)
    t = re.sub(r"[ \t]+\n", "\n", t)
    return t.strip()

# --------------------------------
# (ì„ íƒ) HF Inference APIë¡œ í•œêµ­ì–´ NER í˜¸ì¶œ
# --------------------------------
def hf_ner(text: str, model_id: str, token: str) -> List[Dict[str, Any]]:
    """
    Hugging Face Inference API Token Classification í˜¸ì¶œ.
    ë°˜í™˜ ì˜ˆ: [{"entity_group":"PER","word":"í™ê¸¸ë™",...}, ...]
    """
    api_url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {"Authorization": f"Bearer {token}"} if token else {}
    payload = {
        "inputs": text,
        "parameters": {"aggregation_strategy": "simple"},
        "options": {"wait_for_model": True},
    }
    resp = requests.post(api_url, headers=headers, json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()

    # ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë‹¤ì–‘í•œ í¬ë§· ëŒ€ì‘ â†’ í‰íƒ„í™”
    flat: List[Any] = []
    def _flatten(x):
        if isinstance(x, list):
            for e in x:
                _flatten(e)
        else:
            flat.append(x)

    if isinstance(data, dict) and "error" in data:
        raise RuntimeError(data["error"])
    _flatten(data)
    return flat

def extract_person_aliases_by_ner(text: str, model_id: str, token: str, top_k: int = 10) -> List[str]:
    ents = hf_ner(text, model_id, token)
    persons = []
    for e in ents:
        group = (e.get("entity_group") or e.get("entity") or "").upper()
        if group.startswith("PER"):
            w = (e.get("word") or "").strip("# ").strip()
            if len(w) >= 2:
                persons.append(w)
    if not persons:
        return []
    s = pd.Series(persons).value_counts()
    return list(s.head(top_k).index)

# --------------------------------
# ì‚¬ì´ë“œë°” ì˜µì…˜
# --------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    mode = st.radio("ë“±ì¥ì¸ë¬¼ ì¶”ì¶œ ëª¨ë“œ", ["ë£°ê¸°ë°˜(ê¶Œì¥)", "Hugging Face NER(API)"], help="ìˆ˜ì—…/ë°°í¬ ì•ˆì •ì„±ì€ ë£°ê¸°ë°˜ì´ ì¢‹ìŠµë‹ˆë‹¤.")
    window = st.number_input("ìœˆë„ìš°(ë¬¸ì¥ ìˆ˜)", min_value=5, max_value=200, value=20, step=1)
    step = st.number_input("ìŠ¬ë¼ì´ë“œ ê°„ê²©(ë¬¸ì¥ ìˆ˜)", min_value=1, max_value=200, value=10, step=1)
    show_mode = st.selectbox("ì°¨íŠ¸ ì¢…ë¥˜", ["ì„ í˜•(Line)", "ë©´ì (Stacked Area)"])
    smooth = st.checkbox("ë¶€ë“œëŸ¬ìš´ ê³¡ì„ (spline)", value=True)
    show_counts = st.checkbox("ë“±ì¥ ë¹„ìœ¨ê³¼ í•¨ê»˜ ê°œìˆ˜ë„ íˆ´íŒì— í‘œì‹œ", value=True)

    st.markdown("---")
    st.subheader("ë°ì´í„° ì…ë ¥")
    txt_file = st.file_uploader(".txt ì—…ë¡œë“œ (ì„ íƒ)", type=["txt"])
    default_demo = st.checkbox("ë°ëª¨ìš© ì§§ì€ í…ìŠ¤íŠ¸ ìë™ ì‚½ì…", value=False)

# --------------------------------
# ë³¸ë¬¸ ì…ë ¥ë¶€
# --------------------------------
text_default = (
    "ì˜¤ëŠ˜ì„ ì–¼ë§ˆë‚˜ ê¸°ë‹¤ë ¸ëŠ”ì§€ ëª¨ë¥¸ë‹¤. ë‚˜ëŠ” ì§€ë‚œë°¤ ì‹¸ ë†“ì€ ì§ì„ ë‹¤ì‹œ ì ê²€í–ˆë‹¤. "
    "ì„¸ë©´ë„êµ¬, ìˆ˜ê±´, íœ´ëŒ€í° ì¶©ì „ê¸°, ê·¸ë¦¬ê³  ì–´ì ¯ë°¤ ì€ìˆ˜ì—ê²Œì„œ ë¹Œë¦° ì±…. "
    "ì€ìˆ˜ëŠ” ì–´ì ¯ë°¤ ë‚´ê²Œ ë§í–ˆë‹¤. 'ë¯¼ìˆ˜ì•¼, ê¼­ ëŒì•„ì™€.' ë‚˜ëŠ” ì›ƒìœ¼ë©° ëŒ€ë‹µí–ˆë‹¤. "
    "ë²„ìŠ¤ ì •ë¥˜ì¥ì—ëŠ” ì§„í˜¸ê°€ ì„œ ìˆì—ˆë‹¤. ì§„í˜¸ëŠ” ë‚´ ì–´ê¹¨ë¥¼ í†¡ ì¹˜ë©° ë§í–ˆë‹¤. 'ê°€ì.'"
)

if default_demo and not txt_file:
    novel_text = text_default
else:
    novel_text = ""

if txt_file is not None:
    try:
        novel_text = txt_file.read().decode("utf-8")
    except Exception:
        novel_text = txt_file.read().decode("cp949", errors="ignore")

novel_text = st.text_area(
    "ì†Œì„¤ ë³¸ë¬¸ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”",
    value=soft_normalize_whitespace(novel_text),
    height=240,
    help="ê¸´ ë³¸ë¬¸ì¼ìˆ˜ë¡ ë¶„ì„ì´ í’ë¶€í•´ì§‘ë‹ˆë‹¤.",
)

# --------------------------------
# ë“±ì¥ì¸ë¬¼ ì‚¬ì „(ë£°ê¸°ë°˜) / HF NER ì„¤ì •
# --------------------------------
char_map: Dict[str, List[str]] = {}

if mode == "ë£°ê¸°ë°˜(ê¶Œì¥)":
    st.subheader("ë“±ì¥ì¸ë¬¼ ì‚¬ì „")
    st.caption("í˜•ì‹ ì˜ˆì‹œ: ê¹€ë¯¼ìˆ˜=ë¯¼ìˆ˜,ë¯¼ìˆ˜ì”¨ / ë°•ì€ìˆ˜=ì€ìˆ˜,ì€ìˆ˜ì–‘ / ì§„í˜¸=ì§„í˜¸,ì§„í˜¸í˜•")
    aliases_str = st.text_area(
        "ì¸ë¬¼=ë³„ì¹­1,ë³„ì¹­2 í•œ ì¤„ì— í•œ ì¸ë¬¼",
        value="ê¹€ë¯¼ìˆ˜=ë¯¼ìˆ˜\në°•ì€ìˆ˜=ì€ìˆ˜\nì§„í˜¸=ì§„í˜¸",
        height=120
    )
    for line in aliases_str.splitlines():
        if "=" in line:
            name, al = line.split("=", 1)
            al_list = [name.strip()] + [a.strip() for a in al.split(",") if a.strip()]
            char_map[name.strip()] = sorted(set(al_list))
else:
    st.subheader("Hugging Face NER ì„¤ì •")
    st.caption("í•œêµ­ì–´ NER ëª¨ë¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'kogpt/bert-base-ner-korean' ë“±). í† í°ì€ .env ë˜ëŠ” ì•„ë˜ì— ì…ë ¥.")
    model_id = st.text_input("ëª¨ë¸ ID", value="", placeholder="ì˜ˆ: tner/roberta-base-ner-korean (ëª¨ë¸ë§ˆë‹¤ ìƒì´)")
    hf_token_input = st.text_input("HF_TOKEN (ì„ íƒ, .envê°€ ìˆìœ¼ë©´ ë¹„ì›Œë‘ì„¸ìš”)", type="password", value="")
    eff_token = hf_token_input.strip() or HF_TOKEN
    topk = st.slider("ìë™ ì¶”ì¶œí•  ìƒìœ„ ì¸ë¬¼ ìˆ˜(ë³„ì¹­ í›„ë³´)", 3, 20, 8)
    manual_plus = st.text_input("ì¶”ê°€ë¡œ ê³ ì •í•  ì¸ë¬¼ëª…(ì‰¼í‘œë¡œ)", value="")

    if st.button("NERë¡œ ì¸ë¬¼ ì¶”ì¶œ ì‹¤í–‰", disabled=(not novel_text or not model_id)):
        with st.spinner("NER ì¶”ì¶œ ì¤‘..."):
            try:
                base_aliases = extract_person_aliases_by_ner(novel_text[:15000], model_id, eff_token, top_k=topk)
                if manual_plus.strip():
                    base_aliases.extend([a.strip() for a in manual_plus.split(",") if a.strip()])
                unique = []
                for a in base_aliases:
                    if a not in unique:
                        unique.append(a)
                char_map = {a: [a] for a in unique}
                st.success(f"ì¸ë¬¼ í›„ë³´ {len(unique)}ëª…: {', '.join(unique[:10])}{' ...' if len(unique) > 10 else ''}")
            except Exception as e:
                st.error(f"NER ì‹¤íŒ¨: {e}")
                char_map = {}

    if char_map:
        st.markdown("**ì¶”ì¶œ/í¸ì§‘ìš© ì¸ë¬¼ ì‚¬ì „** (í•œ ì¤„ = ì¸ë¬¼=ë³„ì¹­1,ë³„ì¹­2)")
        seed = "\n".join([f"{k}={','.join(v)}" for k, v in char_map.items()])
        edited = st.text_area("ì¸ë¬¼ ì‚¬ì „ í¸ì§‘", value=seed, height=160, key="ner_alias_edit")
        char_map = {}
        for line in edited.splitlines():
            if "=" in line:
                name, al = line.split("=", 1)
                al_list = [name.strip()] + [a.strip() for a in al.split(",") if a.strip()]
                char_map[name.strip()] = sorted(set(al_list))

# --------------------------------
# ë¶„ì„ ì‹¤í–‰
# --------------------------------
run = st.button("ë¶„ì„ ì‹¤í–‰", type="primary", disabled=not (novel_text and char_map))

if run:
    if not char_map:
        st.warning("ì¸ë¬¼ ì‚¬ì „ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        st.stop()

    sentences = split_sentences_kr(novel_text)
    if len(sentences) == 0:
        st.error("ë¬¸ì¥ ë¶„ë¦¬ ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    spans = make_windows(sentences, window=window, step=step)
    if not spans:
        st.error("ìœˆë„ìš° ì„¤ì •ìœ¼ë¡œ ìƒì„±ëœ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. ìœˆë„ìš°/ê°„ê²©ì„ ì¤„ì—¬ë³´ì„¸ìš”.")
        st.stop()

    # ì¸ë¬¼ë³„ ì •ê·œì‹ ì»´íŒŒì¼
    alias_regex_map = {ch: compile_alias_regex(aliases) for ch, aliases in char_map.items()}

    # ìœˆë„ìš° í…ìŠ¤íŠ¸ êµ¬ì„± ë° ì¹´ìš´íŠ¸
    rows = []
    for idx, (s, e) in enumerate(spans, start=1):
        chunk_text = " ".join(sentences[s:e])
        counts = {ch: count_mentions(chunk_text, alias_regex_map[ch]) for ch in char_map}
        total = sum(counts.values())
        for ch, c in counts.items():
            ratio = (c / total) if total > 0 else 0.0
            rows.append({
                "window_index": idx,
                "start_sent": s,
                "end_sent": e,
                "character": ch,
                "count": c,
                "ratio": ratio,
                "total_in_window": total,
            })

    df = pd.DataFrame(rows)
    st.success(f"ë¬¸ì¥ ìˆ˜: {len(sentences)} Â· ìœˆë„ìš° ê°œìˆ˜: {len(spans)} Â· ì¸ë¬¼ ìˆ˜: {len(char_map)}")

    # ----------------------
    # ì°¨íŠ¸
    # ----------------------
    st.subheader("ì‹œê°í™”")

    characters = sorted(df["character"].unique().tolist())

    # âœ… fig ê°ì²´ ìƒì„±
    fig = go.Figure()

    if show_mode == "ì„ í˜•(Line)":
        for ch in characters:
            sub = df[df["character"] == ch].sort_values("window_index")
            fig.add_trace(go.Scatter(
                x=sub["window_index"],
                y=sub["ratio"],
                mode="lines+markers",
                name=ch,
                line={"shape": "spline" if smooth else "linear"},
                # âš ï¸ f-string ì•ˆì— %{y}ë¥¼ ë„£ì§€ ë§ ê²ƒ! â†’ ë¬¸ìì—´ ê²°í•©ìœ¼ë¡œ ì²˜ë¦¬
                hovertemplate=(
                    "ìœˆë„ìš° %{x}<br>" +
                    f"{ch} ë¹„ìœ¨: " + "%{y:.2%}<br>" +
                    ("%{customdata}íšŒ / ì´ %{meta}íšŒ" if show_counts else "")
                ),
                customdata=sub["count"] if show_counts else None,
                meta=sub["total_in_window"] if show_counts else None,
            ))
        fig.update_layout(
            xaxis_title="ì†Œì„¤ ì§„í–‰ (ìœˆë„ìš° ì¸ë±ìŠ¤)",
            yaxis_title="ë“±ì¥ ë¹„ìœ¨",
            yaxis_tickformat=".0%",
            template="plotly_white",
            legend_title="ì¸ë¬¼",
            margin=dict(l=40, r=20, t=10, b=40),
        )
    else:
        # ëˆ„ì  ë©´ì  ê·¸ë˜í”„
        for ch in characters:
            sub = df[df["character"] == ch].sort_values("window_index")
            fig.add_trace(go.Scatter(
                x=sub["window_index"],
                y=sub["ratio"],
                stackgroup="one",
                mode="lines",
                name=ch,
                line={"shape": "spline" if smooth else "linear"},
                hovertemplate=(
                    "ìœˆë„ìš° %{x}<br>" +
                    f"{ch} ë¹„ìœ¨: " + "%{y:.2%}<br>" +
                    ("%{customdata}íšŒ / ì´ %{meta}íšŒ" if show_counts else "")
                ),
                customdata=sub["count"] if show_counts else None,
                meta=sub["total_in_window"] if show_counts else None,
            ))
        fig.update_layout(
            xaxis_title="ì†Œì„¤ ì§„í–‰ (ìœˆë„ìš° ì¸ë±ìŠ¤)",
            yaxis_title="ë“±ì¥ ë¹„ìœ¨(ëˆ„ì )",
            yaxis_tickformat=".0%",
            template="plotly_white",
            legend_title="ì¸ë¬¼",
            margin=dict(l=40, r=20, t=10, b=40),
        )

    st.plotly_chart(fig, use_container_width=True)

    # ----------------------
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    # ----------------------
    st.subheader("ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button(
        "CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name="character_timeline.csv",
        mime="text/csv",
    )

    with st.expander("ìœˆë„ìš° ìš”ì•½ í…Œì´ë¸” ë³´ê¸°"):
        table = (
            df.pivot_table(index="window_index", columns="character", values="ratio", aggfunc="first")
            .fillna(0.0)
        )
        st.dataframe(table.style.format("{:.1%}"))

st.markdown("---")
st.markdown("â“’ ë¯¸ë¦¼ë§ˆì´ìŠ¤í„°ê³  1í•™ë…„ 4ë°˜ 2ì¡° **ì†ëˆˆì¹ì¡°** Â· ìˆ˜ì—… ëª©ì ì˜ êµìœ¡ìš© í…œí”Œë¦¿")
